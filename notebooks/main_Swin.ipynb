{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmed\\anaconda3\\envs\\vstp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No CUDA runtime is found, using CUDA_HOME='C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.7'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "# # Inside the /Multimodal-Deep-Regression/notebooks\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from util.utilities import train, evaluate, get_device\n",
    "from util.data_utilities import  process_data, get_train_and_val_loader\n",
    "\n",
    "# import all the models from models module\n",
    "from models import Swin_Transformer\n",
    "# from util.video_swin_transformer import SwinTransformer3D\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing: 6805640750861257989.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmed\\anaconda3\\envs\\vstp\\Lib\\site-packages\\torchvision\\io\\video_reader.py:245: UserWarning: Accurate seek is not implemented for pyav backend\n",
      "  warnings.warn(\"Accurate seek is not implemented for pyav backend\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling video tensors for every 5 frame\n",
      "Original tensor size: (3, 322, 1280, 720)\n",
      "New tensor size: torch.Size([3, 65, 1280, 720])\n",
      "Resized tensor to size: torch.Size([3, 65, 1024, 576])\n",
      "65 100\n",
      "Updated tensor to uniform frame count of: 100\n",
      "Resized tensor to size: torch.Size([3, 100, 1024, 576])\n",
      "6805640750861257989\n",
      "229.7\n",
      "Done processing: ../data/video_packs/video_pack_1000/6805640750861257989.mp4.\n",
      "X Tensor (torch.Size([3, 100, 1024, 576])) saved under: ../data/x_tensors/video_pack_1000/6805640750861257989_x_tensor.pt.\n",
      "Y Tensor (torch.Size([1, 1])) saved under: ../data/y_tensors/video_pack_1000/6805640750861257989_y_tensor.pt.\n",
      "metadata:\n",
      "{'video': {'fps': [30.0], 'duration': [10.733333333333333]}, 'audio': {'framerate': [44100.0], 'duration': [10.796009070294785]}}\n",
      "\n",
      "\n",
      "Currently processing: 6807877755045154053.mp4\n",
      "Downsampling video tensors for every 5 frame\n",
      "Original tensor size: (3, 356, 1024, 576)\n",
      "New tensor size: torch.Size([3, 72, 1024, 576])\n",
      "Resized tensor to size: torch.Size([3, 72, 1024, 576])\n",
      "72 100\n",
      "Updated tensor to uniform frame count of: 100\n",
      "Resized tensor to size: torch.Size([3, 100, 1024, 576])\n",
      "6807877755045154053\n",
      "759.9\n",
      "Done processing: ../data/video_packs/video_pack_1000/6807877755045154053.mp4.\n",
      "X Tensor (torch.Size([3, 100, 1024, 576])) saved under: ../data/x_tensors/video_pack_1000/6807877755045154053_x_tensor.pt.\n",
      "Y Tensor (torch.Size([1, 1])) saved under: ../data/y_tensors/video_pack_1000/6807877755045154053_y_tensor.pt.\n",
      "metadata:\n",
      "{'video': {'fps': [28.054347826086957], 'duration': [12.689655172413794]}, 'audio': {'framerate': [44100.0], 'duration': [12.631995464852608]}}\n",
      "\n",
      "\n",
      "Currently processing: 6818023834596183302.mp4\n",
      "Downsampling video tensors for every 5 frame\n",
      "Original tensor size: (3, 406, 1024, 576)\n",
      "New tensor size: torch.Size([3, 82, 1024, 576])\n",
      "Resized tensor to size: torch.Size([3, 82, 1024, 576])\n",
      "82 100\n",
      "Updated tensor to uniform frame count of: 100\n",
      "Resized tensor to size: torch.Size([3, 100, 1024, 576])\n",
      "6818023834596183302\n",
      "1700.0\n",
      "Done processing: ../data/video_packs/video_pack_1000/6818023834596183302.mp4.\n",
      "X Tensor (torch.Size([3, 100, 1024, 576])) saved under: ../data/x_tensors/video_pack_1000/6818023834596183302_x_tensor.pt.\n",
      "Y Tensor (torch.Size([1, 1])) saved under: ../data/y_tensors/video_pack_1000/6818023834596183302_y_tensor.pt.\n",
      "metadata:\n",
      "{'video': {'fps': [30.0], 'duration': [13.533333333333333]}, 'audio': {'framerate': [44100.0], 'duration': [13.513990929705216]}}\n",
      "\n",
      "\n",
      "Currently processing: 6833672485305732354.mp4\n",
      "Downsampling video tensors for every 5 frame\n",
      "Original tensor size: (3, 743, 1024, 576)\n",
      "New tensor size: torch.Size([3, 149, 1024, 576])\n",
      "Resized tensor to size: torch.Size([3, 149, 1024, 576])\n",
      "149 100\n",
      "Updated tensor to uniform frame count of: 100\n",
      "Resized tensor to size: torch.Size([3, 100, 1024, 576])\n",
      "6833672485305732354\n",
      "12000.0\n",
      "Done processing: ../data/video_packs/video_pack_1000/6833672485305732354.mp4.\n",
      "X Tensor (torch.Size([3, 100, 1024, 576])) saved under: ../data/x_tensors/video_pack_1000/6833672485305732354_x_tensor.pt.\n",
      "Y Tensor (torch.Size([1, 1])) saved under: ../data/y_tensors/video_pack_1000/6833672485305732354_y_tensor.pt.\n",
      "metadata:\n",
      "{'video': {'fps': [25.0], 'duration': [29.72]}, 'audio': {'framerate': [44100.0], 'duration': [29.720997732426305]}}\n",
      "\n",
      "\n",
      "Currently processing: 6838230626076839174.mp4\n",
      "Downsampling video tensors for every 5 frame\n",
      "Original tensor size: (3, 449, 576, 1024)\n",
      "New tensor size: torch.Size([3, 90, 576, 1024])\n",
      "Resized tensor to size: torch.Size([3, 90, 1024, 576])\n",
      "90 100\n",
      "Updated tensor to uniform frame count of: 100\n",
      "Resized tensor to size: torch.Size([3, 100, 1024, 576])\n",
      "6838230626076839174\n",
      "25.9\n",
      "Done processing: ../data/video_packs/video_pack_1000/6838230626076839174.mp4.\n",
      "X Tensor (torch.Size([3, 100, 1024, 576])) saved under: ../data/x_tensors/video_pack_1000/6838230626076839174_x_tensor.pt.\n",
      "Y Tensor (torch.Size([1, 1])) saved under: ../data/y_tensors/video_pack_1000/6838230626076839174_y_tensor.pt.\n",
      "metadata:\n",
      "{'video': {'fps': [30.0], 'duration': [14.966666666666667]}, 'audio': {'framerate': [44100.0], 'duration': [14.929002267573695]}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_data(\n",
    "    input_type='video_pack_1000', \n",
    "    addition_parameters={'first_n_videos': 5}, \n",
    "    verbose=True,\n",
    "    device=device,\n",
    "    skip_frames=True,\n",
    "    frames_to_skip=5,\n",
    "    resize_tensors=True,\n",
    "    uniform_frames=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 100, 1024, 576])\n",
      "torch.Size([3, 100, 1024, 576])\n",
      "4\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = get_train_and_val_loader(\n",
    "    input_type='video_pack_1000',\n",
    "    batch_size=1,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swin_Transformer_model(\n",
      "  (swin_transformer): SwinTransformer3D(\n",
      "    (patch_embed): PatchEmbed3D(\n",
      "      (proj): Conv3d(3, 96, kernel_size=(4, 4, 4), stride=(4, 4, 4))\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock3D(\n",
      "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention3D(\n",
      "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock3D(\n",
      "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention3D(\n",
      "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.018)\n",
      "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
      "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock3D(\n",
      "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention3D(\n",
      "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.036)\n",
      "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock3D(\n",
      "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention3D(\n",
      "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.055)\n",
      "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (2): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock3D(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention3D(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.073)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock3D(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention3D(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.091)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): SwinTransformerBlock3D(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention3D(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.109)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): SwinTransformerBlock3D(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention3D(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.127)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): SwinTransformerBlock3D(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention3D(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.145)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): SwinTransformerBlock3D(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention3D(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.164)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (3): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock3D(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention3D(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.182)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock3D(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention3D(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.200)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=11059200, out_features=1, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n",
      "Total parameters: 38,638,831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmed\\anaconda3\\envs\\vstp\\Lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Forward pass on batch with size: torch.Size([1, 3, 100, 1024, 576])\n",
      "Model Output (shape: torch.Size([1, 1])): tensor([[0.]], grad_fn=<ReluBackward0>)\n",
      "Target (shape: torch.Size([1, 1])): tensor([[759.9000]])\n",
      "\n",
      "\n",
      "Forward pass on batch with size: torch.Size([1, 3, 100, 1024, 576])\n",
      "Model Output (shape: torch.Size([1, 1])): tensor([[0.1228]], grad_fn=<ReluBackward0>)\n",
      "Target (shape: torch.Size([1, 1])): tensor([[1700.]])\n"
     ]
    }
   ],
   "source": [
    "# linear_in_dim work for default network with frames input uniform at 100 and HxW at 1024x576\n",
    "model = Swin_Transformer.Swin_Transformer_model(linear_in_dim=768*25*32*18)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())#, lr=0.01)\n",
    "\n",
    "print(model)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, avg_train_loss = train(model, train_loader, criterion, optimizer, device, verbose=True)\n",
    "    val_loss, avg_val_loss = evaluate(model, val_loader, criterion, device, verbose=True)\n",
    "    \n",
    "    # record the losses\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # print every num times epoch only\n",
    "    num = 1\n",
    "    if ((epoch+1) % num == 0) or epoch == 0:\n",
    "        if epoch == 0:\n",
    "            time_took = (time.time() - start_time) / 60\n",
    "            print(f'First epoch took {time_took:.1f} minutes.')\n",
    "        print(f'Epoch {epoch+1}/{EPOCHS}, Train_Loss: {train_loss:.2f}, Avg: {avg_train_loss:.2f}; Val_Loss: {val_loss:.2f}, Avg: {avg_val_loss:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs7643-a2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
