{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmed\\anaconda3\\envs\\vstp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No CUDA runtime is found, using CUDA_HOME='C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.7'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "# # Inside the /Multimodal-Deep-Regression/notebooks\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from util.utilities import train, evaluate, get_device\n",
    "from util.data_utilities import  process_data, get_train_and_val_loader\n",
    "\n",
    "# import all the models from models module\n",
    "from models import Swin_Transformer\n",
    "# from util.video_swin_transformer import SwinTransformer3D\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing: 6805640750861257989.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmed\\anaconda3\\envs\\vstp\\Lib\\site-packages\\torchvision\\io\\video_reader.py:245: UserWarning: Accurate seek is not implemented for pyav backend\n",
      "  warnings.warn(\"Accurate seek is not implemented for pyav backend\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling video tensors for every 5 frame\n",
      "Original tensor size: (3, 322, 1280, 720)\n",
      "New tensor size: torch.Size([3, 65, 1280, 720])\n",
      "6805640750861257989\n",
      "229.7\n",
      "Done processing: ../data/video_packs/video_pack_1000/6805640750861257989.mp4.\n",
      "X Tensor (torch.Size([3, 65, 1280, 720])) saved under: ../data/x_tensors/video_pack_1000/6805640750861257989_x_tensor.pt.\n",
      "Y Tensor (torch.Size([1, 1])) saved under: ../data/y_tensors/video_pack_1000/6805640750861257989_y_tensor.pt.\n",
      "metadata:\n",
      "{'video': {'fps': [30.0], 'duration': [10.733333333333333]}, 'audio': {'framerate': [44100.0], 'duration': [10.796009070294785]}}\n",
      "\n",
      "\n",
      "Currently processing: 6807877755045154053.mp4\n",
      "Downsampling video tensors for every 5 frame\n",
      "Original tensor size: (3, 356, 1024, 576)\n",
      "New tensor size: torch.Size([3, 72, 1024, 576])\n",
      "6807877755045154053\n",
      "759.9\n",
      "Done processing: ../data/video_packs/video_pack_1000/6807877755045154053.mp4.\n",
      "X Tensor (torch.Size([3, 72, 1024, 576])) saved under: ../data/x_tensors/video_pack_1000/6807877755045154053_x_tensor.pt.\n",
      "Y Tensor (torch.Size([1, 1])) saved under: ../data/y_tensors/video_pack_1000/6807877755045154053_y_tensor.pt.\n",
      "metadata:\n",
      "{'video': {'fps': [28.054347826086957], 'duration': [12.689655172413794]}, 'audio': {'framerate': [44100.0], 'duration': [12.631995464852608]}}\n",
      "\n",
      "\n",
      "Currently processing: 6818023834596183302.mp4\n",
      "Downsampling video tensors for every 5 frame\n",
      "Original tensor size: (3, 406, 1024, 576)\n",
      "New tensor size: torch.Size([3, 82, 1024, 576])\n",
      "6818023834596183302\n",
      "1700.0\n",
      "Done processing: ../data/video_packs/video_pack_1000/6818023834596183302.mp4.\n",
      "X Tensor (torch.Size([3, 82, 1024, 576])) saved under: ../data/x_tensors/video_pack_1000/6818023834596183302_x_tensor.pt.\n",
      "Y Tensor (torch.Size([1, 1])) saved under: ../data/y_tensors/video_pack_1000/6818023834596183302_y_tensor.pt.\n",
      "metadata:\n",
      "{'video': {'fps': [30.0], 'duration': [13.533333333333333]}, 'audio': {'framerate': [44100.0], 'duration': [13.513990929705216]}}\n",
      "\n",
      "\n",
      "Currently processing: 6833672485305732354.mp4\n",
      "Downsampling video tensors for every 5 frame\n",
      "Original tensor size: (3, 743, 1024, 576)\n",
      "New tensor size: torch.Size([3, 149, 1024, 576])\n",
      "6833672485305732354\n",
      "12000.0\n",
      "Done processing: ../data/video_packs/video_pack_1000/6833672485305732354.mp4.\n",
      "X Tensor (torch.Size([3, 149, 1024, 576])) saved under: ../data/x_tensors/video_pack_1000/6833672485305732354_x_tensor.pt.\n",
      "Y Tensor (torch.Size([1, 1])) saved under: ../data/y_tensors/video_pack_1000/6833672485305732354_y_tensor.pt.\n",
      "metadata:\n",
      "{'video': {'fps': [25.0], 'duration': [29.72]}, 'audio': {'framerate': [44100.0], 'duration': [29.720997732426305]}}\n",
      "\n",
      "\n",
      "Currently processing: 6838230626076839174.mp4\n",
      "Downsampling video tensors for every 5 frame\n",
      "Original tensor size: (3, 449, 576, 1024)\n",
      "New tensor size: torch.Size([3, 90, 576, 1024])\n",
      "6838230626076839174\n",
      "25.9\n",
      "Done processing: ../data/video_packs/video_pack_1000/6838230626076839174.mp4.\n",
      "X Tensor (torch.Size([3, 90, 576, 1024])) saved under: ../data/x_tensors/video_pack_1000/6838230626076839174_x_tensor.pt.\n",
      "Y Tensor (torch.Size([1, 1])) saved under: ../data/y_tensors/video_pack_1000/6838230626076839174_y_tensor.pt.\n",
      "metadata:\n",
      "{'video': {'fps': [30.0], 'duration': [14.966666666666667]}, 'audio': {'framerate': [44100.0], 'duration': [14.929002267573695]}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_data(\n",
    "    input_type='video_pack_1000', \n",
    "    addition_parameters={'first_n_videos': 5}, \n",
    "    verbose=True,\n",
    "    device=device,\n",
    "    skip_frames=True,\n",
    "    frames_to_skip=5,\n",
    "    resize_tensors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 65, 1280, 720])\n",
      "torch.Size([3, 90, 576, 1024])\n",
      "4\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = get_train_and_val_loader(\n",
    "    input_type='video_pack_1000',\n",
    "    batch_size=1,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 27,579,632\n",
      "\n",
      "\n",
      "Forward pass on batch with size: torch.Size([1, 3, 65, 1280, 720])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmed\\anaconda3\\envs\\vstp\\Lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinTransformer3D(\n",
      "  (patch_embed): PatchEmbed3D(\n",
      "    (proj): Conv3d(3, 96, kernel_size=(4, 4, 4), stride=(4, 4, 4))\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0): BasicLayer(\n",
      "      (blocks): ModuleList(\n",
      "        (0): SwinTransformerBlock3D(\n",
      "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention3D(\n",
      "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinTransformerBlock3D(\n",
      "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention3D(\n",
      "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.018)\n",
      "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (downsample): PatchMerging(\n",
      "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
      "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicLayer(\n",
      "      (blocks): ModuleList(\n",
      "        (0): SwinTransformerBlock3D(\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention3D(\n",
      "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.036)\n",
      "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinTransformerBlock3D(\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention3D(\n",
      "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.055)\n",
      "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (downsample): PatchMerging(\n",
      "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (2): BasicLayer(\n",
      "      (blocks): ModuleList(\n",
      "        (0): SwinTransformerBlock3D(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention3D(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.073)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinTransformerBlock3D(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention3D(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.091)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): SwinTransformerBlock3D(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention3D(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.109)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): SwinTransformerBlock3D(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention3D(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.127)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): SwinTransformerBlock3D(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention3D(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.145)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): SwinTransformerBlock3D(\n",
      "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention3D(\n",
      "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.164)\n",
      "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (downsample): PatchMerging(\n",
      "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (3): BasicLayer(\n",
      "      (blocks): ModuleList(\n",
      "        (0): SwinTransformerBlock3D(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention3D(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.182)\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinTransformerBlock3D(\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): WindowAttention3D(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (drop_path): DropPath(drop_prob=0.200)\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "torch.Size([1, 768, 17, 40, 23])\n",
      "torch.Size([1, 12011520])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x12011520 and 1x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[1;32m---> 15\u001b[0m     train_loss, avg_train_loss \u001b[39m=\u001b[39m train(model, train_loader, criterion, optimizer, device, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     16\u001b[0m     val_loss, avg_val_loss \u001b[39m=\u001b[39m evaluate(model, val_loader, criterion, device, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     18\u001b[0m     \u001b[39m# record the losses\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\Documents\\DeepLearningSummer\\Multimodal-Deep-Regression\\notebooks\\..\\util\\utilities.py:21\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, criterion, optimizer, device, verbose)\u001b[0m\n\u001b[0;32m     19\u001b[0m     inputs, targets \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32), targets\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     20\u001b[0m     \u001b[39m# forward pass\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     23\u001b[0m \u001b[39m# except Exception as e:\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39m#     print(f'Unable to train on tensor of size: {inputs.shape}')\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m#     print('caugh an error, keep going ' + str(e))\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39m#     untrainable_tensors += 1\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39m#     continue\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39mif\u001b[39;00m outputs\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m targets\u001b[39m.\u001b[39mshape:\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\anaconda3\\envs\\vstp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\Documents\\DeepLearningSummer\\Multimodal-Deep-Regression\\notebooks\\..\\models\\Swin_Transformer.py:31\u001b[0m, in \u001b[0;36mSwin_Transformer_model.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten(x)\n\u001b[0;32m     30\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 31\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mregressor(x)\n\u001b[0;32m     32\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\anaconda3\\envs\\vstp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\anaconda3\\envs\\vstp\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x12011520 and 1x1)"
     ]
    }
   ],
   "source": [
    "model = Swin_Transformer.Swin_Transformer_model()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())#, lr=0.01)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, avg_train_loss = train(model, train_loader, criterion, optimizer, device, verbose=True)\n",
    "    val_loss, avg_val_loss = evaluate(model, val_loader, criterion, device, verbose=True)\n",
    "    \n",
    "    # record the losses\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # print every num times epoch only\n",
    "    num = 1\n",
    "    if ((epoch+1) % num == 0) or epoch == 0:\n",
    "        if epoch == 0:\n",
    "            time_took = (time.time() - start_time) / 60\n",
    "            print(f'First epoch took {time_took:.1f} minutes.')\n",
    "        print(f'Epoch {epoch+1}/{EPOCHS}, Train_Loss: {train_loss:.2f}, Avg: {avg_train_loss:.2f}; Val_Loss: {val_loss:.2f}, Avg: {avg_val_loss:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs7643-a2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
