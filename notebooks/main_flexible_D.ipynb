{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Inside the /Multimodal-Deep-Regression/notebooks\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from util.utilities import train, evaluate\n",
    "\n",
    "# import all the models from models module\n",
    "from models import CNN3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/louis/Documents/gatech/Summer2023/Multimodal-Deep-Regression/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Current Path\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_dir = \"../data/x_tensors/\"\n",
    "y_dir = \"../data/y_tensors/\"\n",
    "\n",
    "os.makedirs(x_dir, exist_ok=True)\n",
    "os.makedirs(y_dir, exist_ok=True)\n",
    "\n",
    "# (Batch, Channels, Frames, Height, Width)\n",
    "# B x C x D x H x W\n",
    "C = 3\n",
    "H = 1024\n",
    "W = 576\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    # random numbers of frames\n",
    "    D = random.randint(100, 300)\n",
    "    x = torch.randint(0, 255, (C, D, H, W))\n",
    "    y = x.to(torch.float32).mean() * 100\n",
    "    \n",
    "    # add a dim\n",
    "    y = [y]\n",
    "    # Save the x & y tensors\n",
    "    torch.save(x, os.path.join(x_dir, f'gen_data_x_{i}.pt'))\n",
    "    torch.save(y, os.path.join(y_dir, f'gen_data_y_{i}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128, 1024, 576])\n",
      "torch.Size([3, 218, 1024, 576])\n",
      "8\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "x_dir = \"../data/x_tensors/\"\n",
    "y_dir = \"../data/y_tensors/\"\n",
    "\n",
    "x_files = sorted([os.path.join(x_dir, f) for f in os.listdir(x_dir)])\n",
    "y_files = sorted([os.path.join(y_dir, f) for f in os.listdir(y_dir)])\n",
    "\n",
    "# load all tensors\n",
    "x_data = [torch.load(f) for f in x_files]\n",
    "y_data = [torch.load(f) for f in y_files]\n",
    "\n",
    "# list to tenors, cannot stack different depth or size\n",
    "#x_data = torch.stack(x_data)\n",
    "#y_data = torch.stack(y_data)\n",
    "\n",
    "# Split the data\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, shuffle=False)\n",
    "print(x_train[0].size())\n",
    "print(x_val[0].size())\n",
    "print(len(y_train))\n",
    "print(len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    \n",
    "    # max depth of each batch\n",
    "    max_d = max([x.shape[1] for x, y in batch])\n",
    "    padded_x = []\n",
    "    y_batch = []\n",
    "\n",
    "    for x, y in batch:\n",
    "        d = x.shape[1]\n",
    "        \n",
    "        # ConstantPad3d (left, right, top, bottom, front, back)\n",
    "        padding = nn.ConstantPad3d((0, 0, 0, 0, 0, max_d - d), 0)\n",
    "        padded_x.append(padding(x))\n",
    "        y_batch.append(y)\n",
    "\n",
    "    x = torch.stack(padded_x)\n",
    "    y = torch.tensor(y_batch)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Batches with DataLoaders\n",
    "batch_size = 1\n",
    "train_loader = list(zip(x_train, y_train)) #TensorDataset(x_train, y_train)\n",
    "val_loader = list(zip(x_val, y_val)) #TensorDataset(x_val, y_val)\n",
    "train_loader = DataLoader(train_loader, batch_size=batch_size, shuffle=True, collate_fn=generate_batch)\n",
    "val_loader = DataLoader(val_loader, batch_size=batch_size, shuffle=False, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 603981089\n",
      "First epoch took -4.937513474623362 minutes.\n",
      "Epoch 1/20, Train_Loss: 4152587790433184.00, Avg: 519073473804148.00; Val_Loss: 40103462305792.00, Avg: 20051731152896.00\n",
      "Epoch 2/20, Train_Loss: 238356179419136.00, Avg: 29794522427392.00; Val_Loss: 59923626983424.00, Avg: 29961813491712.00\n",
      "Epoch 3/20, Train_Loss: 97150176788480.00, Avg: 12143772098560.00; Val_Loss: 1386889609216.00, Avg: 693444804608.00\n",
      "Epoch 4/20, Train_Loss: 2419769735168.00, Avg: 302471216896.00; Val_Loss: 32123471872.00, Avg: 16061735936.00\n",
      "Epoch 5/20, Train_Loss: 269147086208.00, Avg: 33643385776.00; Val_Loss: 3340076928.00, Avg: 1670038464.00\n"
     ]
    }
   ],
   "source": [
    "model = CNN3D()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())#, lr=0.01)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, avg_train_loss = train(model, train_loader, criterion, optimizer)\n",
    "    val_loss, avg_val_loss = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    # record the losses\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # print every num times epoch only\n",
    "    num = 1\n",
    "    if ((epoch+1) % num == 0) or epoch == 0:\n",
    "        if epoch == 0:\n",
    "            time_took = (time.time() - start_time) / 60\n",
    "            print(f'First epoch took { time_took } minutes.')\n",
    "        print(f'Epoch {epoch+1}/{EPOCHS}, Train_Loss: {train_loss:.2f}, Avg: {avg_train_loss:.2f}; Val_Loss: {val_loss:.2f}, Avg: {avg_val_loss:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# lost plot\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to validation set\n",
    "val_values = []\n",
    "predicted_values = []\n",
    "\n",
    "for inputs, targets in val_loader:\n",
    "    inputs, targets = inputs.to(torch.float32), targets.to(torch.float32)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Append the values\n",
    "    val_values.extend(targets.tolist())\n",
    "    predicted_values.extend(outputs.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(val_values, label='Test Values')\n",
    "plt.plot(predicted_values, label='Predicted Values')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Test vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_values, predicted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
