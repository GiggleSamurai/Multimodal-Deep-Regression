{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Install virtual enviroment outside project folder\n",
    "In command line enter:\n",
    "python3 -m venv multimodal\n",
    "python3 -m ipykernel install --user --name=multimodal\n",
    "Start `Jupyter Lab` at /Multimodal-Deep-Regression/notebooks\n",
    "\"\"\"\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import whisper\n",
    "import numpy\n",
    "import warnings\n",
    "\n",
    "# Inside the /Multimodal-Deep-Regression/notebooks\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from util.utilities import train, evaluate, get_device\n",
    "from util.data_utilities import get_base_tensor_directories, generate_batch, process_data, add_ae_tensor, get_ensemble_data, generate_batch_ensemble, generate_autoencoder_batch\n",
    "from util.audio_utilities import extract_embeddings, extract_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing paramemter & ConvLSTMAutoencoder\n",
    "DEVICE = 'cpu' #get_device() # CPU or GPU\n",
    "DATASET = 'video_pack_1000'\n",
    "FRAME_SKIP = 200 # how many frame to skip, reduce depth\n",
    "SHRINK = 8 # shrink the scale (H x W)//N\n",
    "NORMALIZE = False # normalize the pixel to 0 to 1\n",
    "PAD_ALL = False # pad all tensors with max depth\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvLSTMAutoencoder Hyper-Parameters\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 3\n",
    "HIDDEN_SIZE = 64 # ConvLSTMAutoencoder hidden size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Visual & Audio Hyper-Parameters\n",
    "NUM_HEADS = 8 # number of attenion heads\n",
    "HIDDEN_DIM = 256 # Transformer hidden size\n",
    "NUM_LAYERS = 6 # number of Transformer layers\n",
    "\n",
    "# EnsembleModel Hyper-Parameters\n",
    "LEARNING = 1e-3\n",
    "THE_EPOCHS = 3\n",
    "AUDIO_TRANSFORMER = True # if False, will skip audio transfomer part #NOT WORKING YET\n",
    "LATE_FUSION = True # early fusion or late fusion #NOT WORKING YET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract audio from video dataset\n",
    "extract_audio(f\"../data/video_packs/{DATASET}/\", \"../data/audio/\")\n",
    "\n",
    "# using whipser to transcribe audio dialog & extract LLMs embedding\n",
    "extract_embeddings(audio_file_path=\"../data/audio/\", output_dir=\"../data/audio_embeddings/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable pyav warning\n",
    "warnings.filterwarnings(\"ignore\", message=\"Accurate seek is not implemented for pyav backend\")\n",
    "\n",
    "# video visual processing\n",
    "process_data(\n",
    "    input_type=DATASET, \n",
    "    #addition_parameters={'first_n_videos': 10}, \n",
    "    verbose=False,\n",
    "    device=get_device(),\n",
    "    frames_to_skip=FRAME_SKIP,\n",
    "    shrink=SHRINK,\n",
    "    normalize=NORMALIZE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dir, y_dir = get_base_tensor_directories(input_type=DATASET)\n",
    "\n",
    "x_files = sorted([os.path.join(x_dir, f) for f in os.listdir(x_dir)]) \n",
    "y_files = sorted([os.path.join(y_dir, f) for f in os.listdir(y_dir)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all visual tensors\n",
    "x_data = [torch.load(f) for f in x_files] \n",
    "y_data = [torch.load(f) for f in y_files] \n",
    "\n",
    "# split the data\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, shuffle=False)\n",
    "print(x_train[0].size())\n",
    "print(x_val[0].size())\n",
    "print(len(y_train))\n",
    "print(len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AutoEncoder Batches with DataLoaders\n",
    "batch_size = BATCH_SIZE\n",
    "train_loader = list(zip(x_train, x_train))\n",
    "val_loader = list(zip(x_val, x_val))\n",
    "train_loader = DataLoader(train_loader, batch_size=batch_size, shuffle=False, collate_fn=generate_autoencoder_batch)\n",
    "val_loader = DataLoader(val_loader, batch_size=batch_size, shuffle=False, collate_fn=generate_autoencoder_batch)\n",
    "\n",
    "# check size Batch, Channel, Frame, Height, Width\n",
    "data, targets = next(iter(train_loader))\n",
    "data.size(), targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ConvLSTMAutoencoder\n",
    "\n",
    "autoencoder = ConvLSTMAutoencoder(hidden_dim=HIDDEN_SIZE, shrink=SHRINK, normalize=NORMALIZE)\n",
    "autoencoder = autoencoder.to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "total_params = sum(p.numel() for p in autoencoder.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ECHO = 1 # print in N epoch only when training\n",
    "SAVE_NAME = 'ConvLSTMAutoencoder_hidden62_weights.pt' # save the weights\n",
    "autoencoder_load = False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# load check point\n",
    "weights_file = f'../models/save/{SAVE_NAME}'\n",
    "autoencoder.load_state_dict(torch.load(weights_file)['model_state_dict'])\n",
    "autoencoder_load = True\n",
    "train_losses = torch.load(weights_file)['train_losses']\n",
    "val_losses = torch.load(weights_file)['val_losses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "if not autoencoder_load:\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, avg_train_loss = train(autoencoder, train_loader, criterion, optimizer, DEVICE)\n",
    "    val_loss, avg_val_loss = evaluate(autoencoder, val_loader, criterion, DEVICE)\n",
    "    \n",
    "    # record the losses\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # print every num times epoch only\n",
    "    echo = ECHO\n",
    "    if ((epoch+1) % echo == 0) or epoch == 0:\n",
    "        if epoch == 0:\n",
    "            time_took = (time.time() - start_time) / 60\n",
    "            print(f'First epoch took {time_took:.1f} minutes.')\n",
    "        print(f'Epoch {epoch+1}/{EPOCHS}, Train_Loss: {train_loss:.2f}, Avg: {avg_train_loss:.2f}; Val_Loss: {val_loss:.2f}, Avg: {avg_val_loss:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not autoencoder_load:\n",
    "    # save model if better or not exists\n",
    "    model_weights = {'model_state_dict': autoencoder.state_dict(), 'val_loss': avg_train_loss, 'train_losses':train_losses, 'val_losses':val_losses}\n",
    "    weights_file = f'../models/save/{SAVE_NAME}'\n",
    "    if not os.path.isfile(weights_file):\n",
    "        # save new\n",
    "        torch.save(model_weights, weights_file)\n",
    "        print('save new model')\n",
    "    elif model_weights['val_loss'] < torch.load(weights_file)['val_loss']:\n",
    "        # replace\n",
    "        torch.save(model_weights, weights_file)\n",
    "        print('replace old model')\n",
    "    else:\n",
    "        print('old model perform better')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss plot\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sample inspection\n",
    "random_num = random.randint(0, len(val_loader)-1)\n",
    "\n",
    "for i, (inputs, targets) in enumerate(val_loader):\n",
    "    if i == random_num:  # random sample\n",
    "        inputs, targets = inputs.to(torch.float32), targets.to(torch.float32)\n",
    "        outputs = autoencoder(inputs).detach()\n",
    "        break\n",
    "\n",
    "        \n",
    "# Actual\n",
    "print('Actual:')\n",
    "for i in range(3):\n",
    "    # select first 3 frame\n",
    "    image_tensor = targets.squeeze()[0,:,i,:,:]\n",
    "    print(image_tensor.size())\n",
    "    numpy_image = image_tensor.detach().numpy()\n",
    "\n",
    "    # imshow (Height, Width, Channels)\n",
    "    numpy_image = numpy_image.transpose((1, 2, 0))\n",
    "\n",
    "    # normalize to 0,1\n",
    "    numpy_image = (numpy_image - numpy_image.min()) / (numpy_image.max() - numpy_image.min())\n",
    "\n",
    "    plt.imshow(numpy_image)\n",
    "    plt.show()\n",
    "    \n",
    "# AutoEncoder\n",
    "print('AutoEncoder:')\n",
    "for i in range(3):\n",
    "    # select first 3 frame\n",
    "    image_tensor = outputs.squeeze()[0,:,i,:,:]\n",
    "    numpy_image = image_tensor.detach().numpy()\n",
    "\n",
    "    # imshow (Height, Width, Channels)\n",
    "    numpy_image = numpy_image.transpose((1, 2, 0))\n",
    "\n",
    "    # normalize to 0,1\n",
    "    numpy_image = (numpy_image - numpy_image.min()) / (numpy_image.max() - numpy_image.min())\n",
    "\n",
    "    plt.imshow(numpy_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ensemble data\n",
    "x_data, y_data = get_ensemble_data(x_files, DATASET)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, shuffle=False)\n",
    "\n",
    "train_loader = list(zip(x_train, y_train))\n",
    "val_loader = list(zip(x_val, y_val))\n",
    "train_loader = DataLoader(train_loader, batch_size=batch_size, shuffle=False, collate_fn=generate_batch_ensemble)\n",
    "val_loader = DataLoader(val_loader, batch_size=batch_size, shuffle=False, collate_fn=generate_batch_ensemble)\n",
    "\n",
    "print(f'Train set size: {len(x_train)}')\n",
    "print(f'Val set size: {len(x_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import TransformerModel_Visual, TransformerModel_Audio, EnsembleModel\n",
    "\n",
    "model1 = TransformerModel_Visual(\n",
    "d_model = 9216,\n",
    "nhead = NUM_HEADS,\n",
    "d_hid = HIDDEN_DIM,\n",
    "nlayers = NUM_LAYERS\n",
    ")\n",
    "\n",
    "model2 = TransformerModel_Audio(\n",
    "d_model = 512,\n",
    "nhead = NUM_HEADS,\n",
    "d_hid = HIDDEN_DIM ,\n",
    "nlayers = NUM_LAYERS\n",
    ")\n",
    "\n",
    "ensemble_model = EnsembleModel(model1,model2)\n",
    "total_params = sum(p.numel() for p in ensemble_model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def ensemble_train(ensemble_model, autoencoder, dataloader, criterion, optimizer, device='cpu', verbose=False):\n",
    "    ensemble_model.train()\n",
    "    total_loss = 0.0\n",
    "    for visuals, audio_embeds, targets in dataloader:\n",
    "        visuals, audio_embeds, targets = visuals.to(device), audio_embeds.to(device), targets.to(device)\n",
    "        visual_embeds = autoencoder.getembedding(visuals).detach()\n",
    "        outputs = ensemble_model.forward(visual_embeds, audio_embeds)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        return total_loss, avg_loss\n",
    "\n",
    "# Evaluate\n",
    "def ensemble_evaluate(ensemble_model, autoencoder, dataloader, criterion, device='cpu', verbose=False):\n",
    "    ensemble_model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for visuals, audio_embeds, targets in dataloader:\n",
    "            visuals, audio_embeds, targets = visuals.to(device), audio_embeds.to(device), targets.to(device)\n",
    "            visual_embeds = autoencoder.getembedding(visuals).detach()\n",
    "            outputs = ensemble_model.forward(visual_embeds, audio_embeds)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            return total_loss, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ECHO = 1  # print in N epoch only when training\n",
    "SAVE_NAME = 'EnsembleModel_hidden512_weights.pt'  # save the weights\n",
    "ensemble_model_load = False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# load check point\n",
    "weights_file = f'../models/save/{SAVE_NAME}'\n",
    "ensemble_model.load_state_dict(torch.load(weights_file)['model_state_dict'])\n",
    "ensemble_model_load = True\n",
    "train_losses = torch.load(weights_file)['train_losses']\n",
    "val_losses = torch.load(weights_file)['val_losses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ensemble_model.parameters(),lr=LEARNING)\n",
    "\n",
    "EPOCHS = THE_EPOCHS\n",
    "start_time = time.time()\n",
    "\n",
    "if not ensemble_model_load:\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, avg_train_loss = ensemble_train(ensemble_model, autoencoder, train_loader, criterion, optimizer, DEVICE)\n",
    "    val_loss, avg_val_loss = ensemble_evaluate(ensemble_model, autoencoder, val_loader, criterion, DEVICE)\n",
    "    \n",
    "    # record the losses\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # print every num times epoch only\n",
    "    num = ECHO\n",
    "    if ((epoch+1) % ECHO == 0) or epoch == 0:\n",
    "        if epoch == 0:\n",
    "            time_took = (time.time() - start_time) / 60\n",
    "            print(f'First epoch took {time_took:.1f} minutes.')\n",
    "        print(f'Epoch {epoch+1}/{EPOCHS}, Train_Loss: {train_loss:.5f}, Avg: {avg_train_loss:.5f}; Val_Loss: {val_loss:.5f}, Avg: {avg_val_loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ensemble_model_load:\n",
    "    # save model if better or not exists\n",
    "    model_weights = {'model_state_dict': ensemble_model.state_dict(), 'val_loss': avg_train_loss, 'train_losses':train_losses, 'val_losses':val_losses}\n",
    "    weights_file = f'../models/save/{SAVE_NAME}'\n",
    "    if not os.path.isfile(weights_file):\n",
    "        # save new\n",
    "        torch.save(model_weights, weights_file)\n",
    "        print('save new model')\n",
    "    elif model_weights['val_loss'] < torch.load(weights_file)['val_loss']:\n",
    "        # replace\n",
    "        torch.save(model_weights, weights_file)\n",
    "        print('replace old model')\n",
    "    else:\n",
    "        print('old model perform better')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss plot\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to validation set\n",
    "val_values = []\n",
    "predicted_values = []\n",
    "    \n",
    "for visuals, audio_embeds, targets in val_loader:\n",
    "    visuals, audio_embeds, targets = visuals.to(DEVICE), audio_embeds.to(DEVICE), targets.to(DEVICE)\n",
    "    visual_embeds = autoencoder.getembedding(visuals).detach()\n",
    "    outputs = ensemble_model.forward(visual_embeds, audio_embeds)\n",
    "    # Append the values\n",
    "    val_values.extend(targets.squeeze().long().tolist())\n",
    "    predicted_values.extend(outputs.squeeze().long().tolist())\n",
    "    \n",
    "plt.scatter(range(len(val_values)), val_values, label='Test Values')\n",
    "plt.scatter(range(len(predicted_values)), predicted_values, label='Predicted Values')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Test vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperenv",
   "language": "python",
   "name": "whisperenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
